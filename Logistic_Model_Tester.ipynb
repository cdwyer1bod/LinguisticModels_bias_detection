{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bias Inducing Word                                  Sentence Pre-Edit\n",
      "5           followed     relatively few parish clergy or laity followed\n",
      "6       denomination  today the church of ireland is after the roman...\n",
      "7        institution  the church of ireland came into existence as a...\n",
      "8          civilians  controversy has arisen numerous times and from...\n",
      "9             naming  the soviets formed a special commission which ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vectorize import get_pos_tag, vectorize_word, vectorize_pos_n, get_freqs\n",
    "\n",
    "train_alldata = pd.read_csv(\"./final_datasets/train-sents-final1.csv\")\n",
    "dev_alldata = pd.read_csv(\"./final_datasets/dev-sents-final1.csv\")\n",
    "test_alldata = pd.read_csv(\"./final_datasets/test-sents-final1.csv\")\n",
    "\n",
    "def data_stripper(arr):\n",
    "    new_data = arr[:]\n",
    "    new_data.columns = ['Bias Inducing Word','De-Biased Word','Sentence Pre-Edit','Sentence Post-Edit']\n",
    "    return new_data[['Bias Inducing Word','Sentence Pre-Edit']]\n",
    "\n",
    "train,dev,test = data_stripper(train_alldata),data_stripper(dev_alldata),data_stripper(test_alldata)\n",
    "print(train[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entailment_sorter(arr, length_entailing_predicate = 1, orderXY=True):\n",
    "    '''\n",
    "    Takes entailment dataset and distills it into usable information. Use params to get\n",
    "    the output you want. X 'word' Y = True means first argument is X, second is Y. False\n",
    "    means first argument is Y and second is X. \n",
    "    If orderXY = True it includes the last 2 headers:\n",
    "    Entailing Predicate, Entailed Predicate, X.Y=T/F Entailing Pred., X.Y=T/F Entailed Pred.\n",
    "    '''\n",
    "    # TODO: what happens when we want a longer length_entailing_predicate?\n",
    "    if orderXY:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                x_arg, y_arg = True, True\n",
    "                if '@R@' in x: x_arg = False\n",
    "                if '@R@' in y: y_arg = False\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@',''), x_arg, y_arg])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate',\n",
    "                                         'X.Y=T/F Entailing Pred.','X.Y=T/F Entailed Pred.'])\n",
    "        return df\n",
    "    else:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@','')])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_lexicon_file = open('./bias-lexicon/bias-lexicon.txt','r')\n",
    "implicatives_file = open('./bias_related_lexicons/implicatives_karttunen1971.txt','r')\n",
    "assertives_file = open('./bias_related_lexicons/assertives_hooper1975.txt','r')\n",
    "factives_file = open('./bias_related_lexicons/factives_hooper1975.txt','r')\n",
    "hedges_file = open('./bias_related_lexicons/hedges_hyland2005.txt','r')\n",
    "other_file = open('./bias_related_lexicons/other_lexicons.txt','r')\n",
    "report_verbs_file = open('./bias_related_lexicons/report_verbs.txt','r')\n",
    "entailments_file = open('./entailments/reverb_global_clsf_all_tncf_lambda_0.1.txt','r')\n",
    "strong_subjectives_file = open('./subjectivity_clues/strongsubj.csv','r')\n",
    "weak_subjectives_file = open('./subjectivity_clues/weaksubj.csv','r')\n",
    "bias_lexicon = bias_lexicon_file.read().strip().split('\\n')\n",
    "assertives = assertives_file.read().strip().split('\\n')[7:]\n",
    "factives = factives_file.read().strip().split('\\n')[7:]\n",
    "hedges = hedges_file.read().strip().split('\\n')[7:]\n",
    "other_lexicon = other_file.read().strip().split('\\n')\n",
    "report_verbs = report_verbs_file.read().strip().split('\\n')[9:]\n",
    "entailments_prestrip = entailments_file.read().strip().split('\\n')\n",
    "\n",
    "# Strong/weak subjectives\n",
    "# TODO: Word, Priorpolarity (PP) headers\n",
    "strong_subjectives = list(set(strong_subjectives_file.read().strip().split('\\n')))\n",
    "weak_subjectives = list(set(weak_subjectives_file.read().strip().split('\\n')))\n",
    "strong_subjectives_withPP = [strong_subjectives[i].split(',') for i in range(len(strong_subjectives))]\n",
    "weak_subjectives_withPP = [weak_subjectives[i].split(',') for i in range(len(weak_subjectives))]\n",
    "strong_subjectives_list, weak_subjectives_list = [], []\n",
    "for ss_row, ws_row in zip(strong_subjectives_withPP, weak_subjectives_withPP):\n",
    "    strong_subjectives_list.append(ss_row[0])\n",
    "    weak_subjectives_list.append(ws_row[0])\n",
    "\n",
    "# Using Entailments function\n",
    "entailments = entailment_sorter(entailments_prestrip, length_entailing_predicate = 1, \n",
    "                                orderXY=True)\n",
    "\n",
    "entailing_predicates = list(entailments['Entailing Predicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInList(dictionaries, word, n_gram):\n",
    "    '''\n",
    "    Pass in array of dictionaries, word under instpection and n_gram of words - \n",
    "    either [3,4,5]-gram length.\n",
    "    Returns True/False vector if word and if surrounding words are in the dictionary. \n",
    "    Vector length is 2 x (# of dictionaries), first T/F is if word is in dictionary, second\n",
    "    T/F if any of the immediately surrounding word(s) is in dictionary.\n",
    "    Make sure you input dictionaries in the correct order.\n",
    "    '''\n",
    "    tf_vector = []\n",
    "    len_ngram, words_ngram = len(n_gram.split()), np.array(n_gram.split())\n",
    "    surrounding_words = []\n",
    "    if len_ngram == 3:\n",
    "        if word == words_ngram[0]: surrounding_words.append(words_ngram[1])\n",
    "        else: surrounding_words.append(words_ngram[-2])\n",
    "    elif len_ngram == 4:\n",
    "        # n_gram is 4 words long, target word is either in position 2 or 3 \n",
    "        word_index = np.where(word == words_ngram)[0]\n",
    "        if 1 in word_index: # target word is 2nd word\n",
    "            surrounding_words.append(words_ngram[0])\n",
    "            surrounding_words.append(words_ngram[2])\n",
    "        elif 2 in word_index: # target word is 3rd word\n",
    "            surrounding_words.append(words_ngram[1])\n",
    "            surrounding_words.append(words_ngram[3])\n",
    "        # only issue is if the target word repeats?\n",
    "    else:\n",
    "        # n_gram is 5 words long, target word is in the middle\n",
    "        surrounding_words.append(words_ngram[1]) \n",
    "        surrounding_words.append(words_ngram[4])\n",
    "\n",
    "    for dictionary in dictionaries:\n",
    "        if word in dictionary: tf_vector.append(True)\n",
    "        else: tf_vector.append(False)\n",
    "        for surrounding_word in surrounding_words:\n",
    "            if surrounding_word in dictionary:\n",
    "                tf_vector.append(True)\n",
    "                break\n",
    "            else:\n",
    "                # If last word in surrounding_words list, then neither word is in dictionary\n",
    "                if surrounding_word == surrounding_words[-1]:\n",
    "                    tf_vector.append(False)\n",
    "    return np.array(tf_vector)\n",
    "\n",
    "def isInBiasLexicon(word,dictionary=bias_lexicon):\n",
    "    if word in dictionary: return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False  True False  True False  True False\n",
      "  True False]\n",
      "True\n",
      "['think', 'believe', 'suppose', 'expect', 'imagine', 'guess', 'seem', 'appear', 'figure', 'acknowledge']\n"
     ]
    }
   ],
   "source": [
    "print(isInList([assertives,factives,hedges,report_verbs,\n",
    "          entailing_predicates,strong_subjectives_list,weak_subjectives_list],\n",
    "         'think','a large think was committed'))\n",
    "print(isInBiasLexicon('murder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'partter'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sep = '='\n",
    "test = 'asdf=partter'\n",
    "re.sub(r'.*\\=','',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "new = []\n",
    "for row in data:\n",
    "    new.append(row[0])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[1],a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pair, tagged_sent):\n",
    "        index, max_index = tagged_sent.index(pair), len(tagged_sent)-1\n",
    "        context = ''\n",
    "        for n in range(-2, 3):\n",
    "            if (index + n < 0) or (index + n > max_index):\n",
    "                continue\n",
    "            else:\n",
    "                context += tagged_sent[index+n][0] + ' '\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_df(df, POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list):\n",
    "    df['tagged_sent'] = df.apply(get_pos_tag, axis = 1)\n",
    "    attiribute_matrix = []\n",
    "    result_vector = []\n",
    "    for row in df.index:\n",
    "        tagged_sent = df['tagged_sent'].iloc[row]\n",
    "        # FIXME: this definition of word may not match definition in 'Bias Inducing Word'\n",
    "        for pair in tagged_sent:\n",
    "            current_vector = []\n",
    "            current_vector.append(vectorize_word(pair[0], WORD_FREQ_DIST))\n",
    "            for n in range(-1,2):\n",
    "                current_vector.append(vectorize_pos_n(pair[1], n, POS_FREQ_DIST))\n",
    "            current_vector.append(isInList(corpus_list, pair[0], get_context(pair, tagged_sent)))\n",
    "            is_bias = pair[0] == df['Bias Inducing Word'].iloc[row]\n",
    "            result_vector.append(is_bias)\n",
    "            attiribute_matrix.append(current_vector)\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_FREQ_DIST, WORD_FREQ_DIST = get_freqs(\"./final_datasets/train-sents-final1.csv\")\n",
    "corpus_list = [assertives,factives,hedges,report_verbs,\n",
    "          entailing_predicates,strong_subjectives_list,weak_subjectives_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Word occurs more than once in n_gram input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-454688185672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOS_FREQ_DIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWORD_FREQ_DIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-f3234fde8f06>\u001b[0m in \u001b[0;36mvectorize_df\u001b[0;34m(df, POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mcurrent_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_pos_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOS_FREQ_DIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mcurrent_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misInList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mis_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bias Inducing Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mresult_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7978cd337192>\u001b[0m in \u001b[0;36misInList\u001b[0;34m(dictionaries, word, n_gram)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwords_ngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# TODO: if target word occurs more than once in n_gram input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word occurs more than once in n_gram input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msurrounding_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msurrounding_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Word occurs more than once in n_gram input"
     ]
    }
   ],
   "source": [
    "vectorize_df(train[:20], POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.where(a==2)\n",
    "1 in b\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
