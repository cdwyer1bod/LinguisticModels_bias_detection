{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bias Inducing Word                                  Sentence Pre-Edit\n",
      "5           followed     relatively few parish clergy or laity followed\n",
      "6       denomination  today the church of ireland is after the roman...\n",
      "7        institution  the church of ireland came into existence as a...\n",
      "8          civilians  controversy has arisen numerous times and from...\n",
      "9             naming  the soviets formed a special commission which ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_alldata = pd.read_csv(\"./final_datasets/train-sents-final1.csv\")\n",
    "dev_alldata = pd.read_csv(\"./final_datasets/dev-sents-final1.csv\")\n",
    "test_alldata = pd.read_csv(\"./final_datasets/test-sents-final1.csv\")\n",
    "\n",
    "def data_stripper(arr):\n",
    "    new_data = arr[:]\n",
    "    new_data.columns = ['Bias Inducing Word','De-Biased Word','Sentence Pre-Edit','Sentence Post-Edit']\n",
    "    return new_data[['Bias Inducing Word','Sentence Pre-Edit']]\n",
    "\n",
    "train,dev,test = data_stripper(train_alldata),data_stripper(dev_alldata),data_stripper(test_alldata)\n",
    "print(train[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entailment_sorter(arr, length_entailing_predicate = 1, orderXY=True):\n",
    "    '''\n",
    "    Takes entailment dataset and distills it into usable information. Use params to get\n",
    "    the output you want. X 'word' Y = True means first argument is X, second is Y. False\n",
    "    means first argument is Y and second is X. \n",
    "    If orderXY = True it includes the last 2 headers:\n",
    "    Entailing Predicate, Entailed Predicate, X.Y=T/F Entailing Pred., X.Y=T/F Entailed Pred.\n",
    "    '''\n",
    "    # TODO: what happens when we want a longer length_entailing_predicate?\n",
    "    if orderXY:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                x_arg, y_arg = True, True\n",
    "                if '@R@' in x: x_arg = False\n",
    "                if '@R@' in y: y_arg = False\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@',''), x_arg, y_arg])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate',\n",
    "                                         'X.Y=T/F Entailing Pred.','X.Y=T/F Entailed Pred.'])\n",
    "        return df\n",
    "    else:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@','')])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_lexicon_file = open('./bias-lexicon/bias-lexicon.txt','r')\n",
    "implicatives_file = open('./bias_related_lexicons/implicatives_karttunen1971.txt','r')\n",
    "assertives_file = open('./bias_related_lexicons/assertives_hooper1975.txt','r')\n",
    "factives_file = open('./bias_related_lexicons/factives_hooper1975.txt','r')\n",
    "hedges_file = open('./bias_related_lexicons/hedges_hyland2005.txt','r')\n",
    "other_file = open('./bias_related_lexicons/other_lexicons.txt','r')\n",
    "report_verbs_file = open('./bias_related_lexicons/report_verbs.txt','r')\n",
    "entailments_file = open('./entailments/reverb_global_clsf_all_tncf_lambda_0.1.txt','r')\n",
    "strong_subjectives_file = open('./subjectivity_clues/strongsubj.csv','r')\n",
    "weak_subjectives_file = open('./subjectivity_clues/weaksubj.csv','r')\n",
    "bias_lexicon = bias_lexicon_file.read().strip().split('\\n')\n",
    "assertives = assertives_file.read().strip().split('\\n')[7:]\n",
    "factives = factives_file.read().strip().split('\\n')[7:]\n",
    "hedges = hedges_file.read().strip().split('\\n')[7:]\n",
    "other_lexicon = other_file.read().strip().split('\\n')\n",
    "report_verbs = report_verbs_file.read().strip().split('\\n')[9:]\n",
    "entailments_prestrip = entailments_file.read().strip().split('\\n')\n",
    "\n",
    "# Strong/weak subjectives\n",
    "# TODO: Word, Priorpolarity (PP) headers\n",
    "strong_subjectives = list(set(strong_subjectives_file.read().strip().split('\\n')))\n",
    "weak_subjectives = list(set(weak_subjectives_file.read().strip().split('\\n')))\n",
    "strong_subjectives_withPP = [strong_subjectives[i].split(',') for i in range(len(strong_subjectives))]\n",
    "weak_subjectives_withPP = [weak_subjectives[i].split(',') for i in range(len(weak_subjectives))]\n",
    "strong_subjectives_list, weak_subjectives_list = [], []\n",
    "for ss_row, ws_row in zip(strong_subjectives_withPP, weak_subjectives_withPP):\n",
    "    strong_subjectives_list.append(ss_row[0])\n",
    "    weak_subjectives_list.append(ws_row[0])\n",
    "\n",
    "# Using Entailments function\n",
    "entailments = entailment_sorter(entailments_prestrip, length_entailing_predicate = 1, \n",
    "                                orderXY=True)\n",
    "\n",
    "entailing_predicates = list(entailments['Entailing Predicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInList(dictionaries, word, n_gram):\n",
    "    '''\n",
    "    Pass in array of dictionaries, word under instpection and n_gram of words - \n",
    "    either [3,4,5]-gram length.\n",
    "    Returns True/False vector if word and if surrounding words are in the dictionary. \n",
    "    Vector length is 2 x (# of dictionaries), first T/F is if word is in dictionary, second\n",
    "    T/F if any of the immediately surrounding word(s) is in dictionary.\n",
    "    Make sure you input dictionaries in the correct order.\n",
    "    '''\n",
    "    tf_vector = []\n",
    "    len_ngram, words_ngram = len(n_gram.split()), np.array(n_gram.split())\n",
    "    surrounding_words = []\n",
    "    if len_ngram == 3:\n",
    "        if word == words_ngram[0]: surrounding_words.append(words_ngram[1])\n",
    "        else: surrounding_words.append(words_ngram[-2])\n",
    "    else:\n",
    "        word_index = np.where(word == words_ngram)[0]\n",
    "        # TODO: if target word occurs more than once in n_gram input\n",
    "        if len(word_index) > 1: raise ValueError('Word occurs more than once in n_gram input')\n",
    "        surrounding_words.append(words_ngram[word_index-1][0])\n",
    "        surrounding_words.append(words_ngram[word_index+1][0])\n",
    "\n",
    "    for dictionary in dictionaries:\n",
    "        if word in dictionary: tf_vector.append(True)\n",
    "        else: tf_vector.append(False)\n",
    "        for surrounding_word in surrounding_words:\n",
    "            if surrounding_word in dictionary:\n",
    "                tf_vector.append(True)\n",
    "                break\n",
    "            else:\n",
    "                # If last word in surrounding_words list, then neither word is in dictionary\n",
    "                if surrounding_word == surrounding_words[-1]:\n",
    "                    tf_vector.append(False)\n",
    "    return np.array(tf_vector)\n",
    "\n",
    "def isInBiasLexicon(word,dictionary=bias_lexicon):\n",
    "    if word in dictionary: return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False  True  True False\n",
      "  True False]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isInList([assertives,factives,hedges,report_verbs,\n",
    "          entailing_predicates,strong_subjectives_list,weak_subjectives_list],\n",
    "         'worries','a kill worries was committed'))\n",
    "print(isInBiasLexicon('murder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'partter'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sep = '='\n",
    "test = 'asdf=partter'\n",
    "re.sub(r'.*\\=','',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "new = []\n",
    "for row in data:\n",
    "    new.append(row[0])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[1],a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
