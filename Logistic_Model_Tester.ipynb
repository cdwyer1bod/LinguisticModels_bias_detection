{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bias Inducing Word                                  Sentence Pre-Edit\n",
      "5           followed     relatively few parish clergy or laity followed\n",
      "6       denomination  today the church of ireland is after the roman...\n",
      "7        institution  the church of ireland came into existence as a...\n",
      "8          civilians  controversy has arisen numerous times and from...\n",
      "9             naming  the soviets formed a special commission which ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vectorize import get_pos_tag, vectorize_word, vectorize_pos_n, get_freqs\n",
    "\n",
    "train_alldata = pd.read_csv(\"./final_datasets/train-sents-final1.csv\")\n",
    "dev_alldata = pd.read_csv(\"./final_datasets/dev-sents-final1.csv\")\n",
    "test_alldata = pd.read_csv(\"./final_datasets/test-sents-final1.csv\")\n",
    "\n",
    "def data_stripper(arr):\n",
    "    new_data = arr[:]\n",
    "    new_data.columns = ['Bias Inducing Word','De-Biased Word','Sentence Pre-Edit','Sentence Post-Edit']\n",
    "    return new_data[['Bias Inducing Word','Sentence Pre-Edit']]\n",
    "\n",
    "train,dev,test = data_stripper(train_alldata),data_stripper(dev_alldata),data_stripper(test_alldata)\n",
    "print(train[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entailment_sorter(arr, length_entailing_predicate = 1, orderXY=True):\n",
    "    '''\n",
    "    Takes entailment dataset and distills it into usable information. Use params to get\n",
    "    the output you want. X 'word' Y = True means first argument is X, second is Y. False\n",
    "    means first argument is Y and second is X. \n",
    "    If orderXY = True it includes the last 2 headers:\n",
    "    Entailing Predicate, Entailed Predicate, X.Y=T/F Entailing Pred., X.Y=T/F Entailed Pred.\n",
    "    '''\n",
    "    # TODO: what happens when we want a longer length_entailing_predicate?\n",
    "    if orderXY:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                x_arg, y_arg = True, True\n",
    "                if '@R@' in x: x_arg = False\n",
    "                if '@R@' in y: y_arg = False\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@',''), x_arg, y_arg])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate',\n",
    "                                         'X.Y=T/F Entailing Pred.','X.Y=T/F Entailed Pred.'])\n",
    "        return df\n",
    "    else:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@','')])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_lexicon_file = open('./bias-lexicon/bias-lexicon.txt','r')\n",
    "implicatives_file = open('./bias_related_lexicons/implicatives_karttunen1971.txt','r')\n",
    "assertives_file = open('./bias_related_lexicons/assertives_hooper1975.txt','r')\n",
    "factives_file = open('./bias_related_lexicons/factives_hooper1975.txt','r')\n",
    "hedges_file = open('./bias_related_lexicons/hedges_hyland2005.txt','r')\n",
    "other_file = open('./bias_related_lexicons/other_lexicons.txt','r')\n",
    "report_verbs_file = open('./bias_related_lexicons/report_verbs.txt','r')\n",
    "entailments_file = open('./entailments/reverb_global_clsf_all_tncf_lambda_0.1.txt','r')\n",
    "strong_subjectives_file = open('./subjectivity_clues/strongsubj.csv','r')\n",
    "weak_subjectives_file = open('./subjectivity_clues/weaksubj.csv','r')\n",
    "bias_lexicon = bias_lexicon_file.read().strip().split('\\n')\n",
    "assertives = assertives_file.read().strip().split('\\n')[7:]\n",
    "factives = factives_file.read().strip().split('\\n')[7:]\n",
    "hedges = hedges_file.read().strip().split('\\n')[7:]\n",
    "other_lexicon = other_file.read().strip().split('\\n')\n",
    "report_verbs = report_verbs_file.read().strip().split('\\n')[9:]\n",
    "entailments_prestrip = entailments_file.read().strip().split('\\n')\n",
    "\n",
    "# Strong/weak subjectives\n",
    "# TODO: Word, Priorpolarity (PP) headers\n",
    "strong_subjectives = list(set(strong_subjectives_file.read().strip().split('\\n')))\n",
    "weak_subjectives = list(set(weak_subjectives_file.read().strip().split('\\n')))\n",
    "strong_subjectives_withPP = [strong_subjectives[i].split(',') for i in range(len(strong_subjectives))]\n",
    "weak_subjectives_withPP = [weak_subjectives[i].split(',') for i in range(len(weak_subjectives))]\n",
    "strong_subjectives_list, weak_subjectives_list = [], []\n",
    "for ss_row, ws_row in zip(strong_subjectives_withPP, weak_subjectives_withPP):\n",
    "    strong_subjectives_list.append(ss_row[0])\n",
    "    weak_subjectives_list.append(ws_row[0])\n",
    "\n",
    "# Using Entailments function\n",
    "entailments = entailment_sorter(entailments_prestrip, length_entailing_predicate = 1, \n",
    "                                orderXY=True)\n",
    "\n",
    "entailing_predicates = list(entailments['Entailing Predicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInList(dictionaries, word, n_gram):\n",
    "    '''\n",
    "    Pass in array of dictionaries, word under instpection and n_gram of words - \n",
    "    either [3,4,5]-gram length.\n",
    "    Returns True/False vector if word and if surrounding words are in the dictionary. \n",
    "    Vector length is 2 x (# of dictionaries), first T/F is if word is in dictionary, second\n",
    "    T/F if any of the immediately surrounding word(s) is in dictionary.\n",
    "    Make sure you input dictionaries in the correct order.\n",
    "    '''\n",
    "    tf_vector = []\n",
    "    len_ngram, words_ngram = len(n_gram.split()), np.array(n_gram.split())\n",
    "    surrounding_words = []\n",
    "    if len_ngram == 3:\n",
    "        if word == words_ngram[0]: surrounding_words.append(words_ngram[1])\n",
    "        else: surrounding_words.append(words_ngram[-2])\n",
    "    else:\n",
    "        word_index = np.where(word == words_ngram)[0]\n",
    "        # TODO: if target word occurs more than once in n_gram input\n",
    "        if len(word_index) > 1: raise ValueError('Word occurs more than once in n_gram input')\n",
    "        surrounding_words.append(words_ngram[word_index-1][0])\n",
    "        surrounding_words.append(words_ngram[word_index+1][0])\n",
    "\n",
    "    for dictionary in dictionaries:\n",
    "        if word in dictionary: tf_vector.append(True)\n",
    "        else: tf_vector.append(False)\n",
    "        for surrounding_word in surrounding_words:\n",
    "            if surrounding_word in dictionary:\n",
    "                tf_vector.append(True)\n",
    "                break\n",
    "            else:\n",
    "                # If last word in surrounding_words list, then neither word is in dictionary\n",
    "                if surrounding_word == surrounding_words[-1]:\n",
    "                    tf_vector.append(False)\n",
    "    return np.array(tf_vector)\n",
    "\n",
    "def isInBiasLexicon(word,dictionary=bias_lexicon):\n",
    "    if word in dictionary: return True\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False  True  True False\n",
      "  True False]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isInList([assertives,factives,hedges,report_verbs,\n",
    "          entailing_predicates,strong_subjectives_list,weak_subjectives_list],\n",
    "         'worries','a kill worries was committed'))\n",
    "print(isInBiasLexicon('murder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'partter'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sep = '='\n",
    "test = 'asdf=partter'\n",
    "re.sub(r'.*\\=','',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "new = []\n",
    "for row in data:\n",
    "    new.append(row[0])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[1],a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pair, tagged_sent):\n",
    "        index, max_index = tagged_sent.index(pair), len(tagged_sent)-1\n",
    "        context = ''\n",
    "        for n in range(-2, 3):\n",
    "            if (index + n < 0) or (index + n > max_index):\n",
    "                continue\n",
    "            else:\n",
    "                context += tagged_sent[index+n][0] + ' '\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_df(df, POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list):\n",
    "    df['tagged_sent'] = df.apply(get_pos_tag, axis = 1)\n",
    "    attiribute_matrix = []\n",
    "    result_vector = []\n",
    "    for row in df.index:\n",
    "        tagged_sent = df['tagged_sent'].iloc[row]\n",
    "        # FIXME: this definition of word may not match definition in 'Bias Inducing Word'\n",
    "        for pair in tagged_sent:\n",
    "            current_vector = []\n",
    "            current_vector.append(vectorize_word(pair[0], WORD_FREQ_DIST))\n",
    "            for n in range(-1,2):\n",
    "                current_vector.append(vectorize_pos_n(pair[1], n, POS_FREQ_DIST))\n",
    "            current_vector.append(isInList(corpus_list, pair[0], get_context(pair, tagged_sent)))\n",
    "            is_bias = pair[0] == df['Bias Inducing Word'].iloc[row]\n",
    "            result_vector.append(is_bias)\n",
    "            attiribute_matrix.append(current_vector)\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_FREQ_DIST, WORD_FREQ_DIST = get_freqs(\"./final_datasets/train-sents-final1.csv\")\n",
    "corpus_list = [assertives,factives,hedges,report_verbs,\n",
    "          entailing_predicates,strong_subjectives_list,weak_subjectives_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Word occurs more than once in n_gram input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-454688185672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorize_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOS_FREQ_DIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWORD_FREQ_DIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-f3234fde8f06>\u001b[0m in \u001b[0;36mvectorize_df\u001b[0;34m(df, POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mcurrent_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_pos_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOS_FREQ_DIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mcurrent_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misInList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mis_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bias Inducing Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mresult_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7978cd337192>\u001b[0m in \u001b[0;36misInList\u001b[0;34m(dictionaries, word, n_gram)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwords_ngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# TODO: if target word occurs more than once in n_gram input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word occurs more than once in n_gram input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0msurrounding_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msurrounding_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_ngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Word occurs more than once in n_gram input"
     ]
    }
   ],
   "source": [
    "vectorize_df(train[:20], POS_FREQ_DIST, WORD_FREQ_DIST, corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias Inducing Word</th>\n",
       "      <th>Sentence Pre-Edit</th>\n",
       "      <th>tagged_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recently</td>\n",
       "      <td>mpri recently used retired military and curren...</td>\n",
       "      <td>[(mpri, NN), (recently, RB), (used, VBD), (ret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>government</td>\n",
       "      <td>now mpri contracts to both local police forces...</td>\n",
       "      <td>[(now, RB), (mpri, VBZ), (contracts, NNS), (to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thesis</td>\n",
       "      <td>the book was criticized by many as a rather gr...</td>\n",
       "      <td>[(the, DT), (book, NN), (was, VBD), (criticize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indeed</td>\n",
       "      <td>indeed sokal bricmont and many of their suppor...</td>\n",
       "      <td>[(indeed, RB), (sokal, JJ), (bricmont, NN), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>the church of ireland claims apostolic success...</td>\n",
       "      <td>[(the, DT), (church, NN), (of, IN), (ireland, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Bias Inducing Word                                  Sentence Pre-Edit  \\\n",
       "0           recently  mpri recently used retired military and curren...   \n",
       "1         government  now mpri contracts to both local police forces...   \n",
       "2             thesis  the book was criticized by many as a rather gr...   \n",
       "3             indeed  indeed sokal bricmont and many of their suppor...   \n",
       "4             denied  the church of ireland claims apostolic success...   \n",
       "\n",
       "                                         tagged_sent  \n",
       "0  [(mpri, NN), (recently, RB), (used, VBD), (ret...  \n",
       "1  [(now, RB), (mpri, VBZ), (contracts, NNS), (to...  \n",
       "2  [(the, DT), (book, NN), (was, VBD), (criticize...  \n",
       "3  [(indeed, RB), (sokal, JJ), (bricmont, NN), (a...  \n",
       "4  [(the, DT), (church, NN), (of, IN), (ireland, ...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
