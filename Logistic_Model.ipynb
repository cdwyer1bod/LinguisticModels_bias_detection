{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bias Inducing Word                                  Sentence Pre-Edit\n",
      "5         Protestant  The Primate of All Ireland is the Archbishop o...\n",
      "6             denied  The Church of Ireland claims Apostolic success...\n",
      "7          followed.    Relatively few parish clergy or laity followed.\n",
      "8      denomination.  Today the Church of Ireland is, after the Roma...\n",
      "9        institution  The Church of Ireland came into existence as a...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_alldata = pd.read_csv(\"./5gram-edits-train-sents-clean.csv\")\n",
    "dev_alldata = pd.read_csv(\"./5gram-edits-dev-sents-clean.csv\")\n",
    "test_alldata = pd.read_csv(\"./5gram-edits-test-sents-clean.csv\")\n",
    "\n",
    "def data_stripper(arr, included_cols=['7','9']):\n",
    "    new_data = arr[included_cols]\n",
    "    new_data.columns = ['Bias Inducing Word', 'Sentence Pre-Edit']\n",
    "    return new_data\n",
    "\n",
    "train,dev,test = data_stripper(train_alldata),data_stripper(dev_alldata),data_stripper(test_alldata)\n",
    "print(train[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sweep', 'revolutionize', 'attribute', 'presume', 'experience', 'undermine', 'recall', 'charge', 'customize', 'contract']\n",
      "654 66 27 100 11 181 18070\n"
     ]
    }
   ],
   "source": [
    "bias_lexicon_file = open('./bias-lexicon/bias-lexicon.txt','r')\n",
    "implicatives_file = open('./bias_related_lexicons/implicatives_karttunen1971.txt','r')\n",
    "assertives_file = open('./bias_related_lexicons/assertives_hooper1975.txt','r')\n",
    "factives_file = open('./bias_related_lexicons/factives_hooper1975.txt','r')\n",
    "hedges_file = open('./bias_related_lexicons/hedges_hyland2005.txt','r')\n",
    "other_file = open('./bias_related_lexicons/other_lexicons.txt','r')\n",
    "report_verbs_file = open('./bias_related_lexicons/report_verbs.txt','r')\n",
    "entailments_file = open('./entailments/reverb_global_clsf_all_tncf_lambda_0.1.txt','r')\n",
    "bias_lexicon = bias_lexicon_file.read().strip().split('\\n')\n",
    "assertives = assertives_file.read().strip().split('\\n')[7:]\n",
    "factives = factives_file.read().strip().split('\\n')[7:]\n",
    "hedges = hedges_file.read().strip().split('\\n')[7:]\n",
    "other_lexicon = other_file.read().strip().split('\\n')\n",
    "report_verbs = report_verbs_file.read().strip().split('\\n')[9:]\n",
    "entailments_prestrip = entailments_file.read().strip().split('\\n')\n",
    "\n",
    "def entailment_sorter(arr, length_entailing_predicate = 1, orderXY=True):\n",
    "    '''\n",
    "    Takes entailment dataset and distills it into usable information. Use params to get\n",
    "    the output you want. X 'word' Y = True means first argument is X, second is Y. False\n",
    "    means first argument is Y and second is X. \n",
    "    If orderXY = True it includes the last 2 headers:\n",
    "    entailing predicate, entailed predicate, X.Y=T/F entailing pred., X.Y=T/F entailed pred.\n",
    "    '''\n",
    "    if orderXY:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                x_arg, y_arg = True, True\n",
    "                if '@R@' in x: x_arg = False\n",
    "                if '@R@' in y: y_arg = False\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@',''), x_arg, y_arg])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate',\n",
    "                                         'X.Y=T/F Entailing Pred.','X.Y=T/F Entailed Pred.'])\n",
    "        return df\n",
    "    else:\n",
    "        data = []\n",
    "        for e in arr:\n",
    "            x, y = e.split('\\t')\n",
    "            if len(x.split()) <= length_entailing_predicate:\n",
    "                data.append([x.replace('@R@',''), y.replace('@R@','')])\n",
    "        df = pd.DataFrame(data, columns=['Entailing Predicate','Entailed Predicate'])\n",
    "        return df\n",
    "\n",
    "entailments = entailment_sorter(entailments_prestrip, length_entailing_predicate = 1, \n",
    "                                orderXY=True)\n",
    "entailing_pred = list(entailments['Entailing Predicate'])\n",
    "\n",
    "print(len(bias_lexicon), len(assertives), len(factives), len(hedges), \n",
    "      len(other_lexicon), len(report_verbs), len(entailing_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_vars(arr,tag_index=10):\n",
    "    POS_tags = np.array(['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS',\n",
    "                'NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','TO','UH',\n",
    "               'VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB'])\n",
    "    \n",
    "    word_tag_tuples = arr[tag_index]\n",
    "    tag_vector = np.zeros(len(POS_tags))\n",
    "    for i in range(len(word_tag_tuples)):\n",
    "        word = word_tag_tuples[i][0]\n",
    "        tag = word_tag_tuples[i][1]\n",
    "        tag_vector_index = np.where(tag == POS_tags)\n",
    "        tag_vector[tag_vector_index] += 1\n",
    "    return tag_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [2,[('Man','NNP'),('Joe','VBN')]]\n",
    "dummy_vars(b,tag_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
